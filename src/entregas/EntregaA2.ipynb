{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fundação Getúlio Vargas - RJ <br>\n",
    "> Escola de Matemática Aplicada (EMAp) <br>\n",
    "> Graduação em Ciência de Dados e Inteligência Artificial <br>\n",
    "> Alunos: Gianlucca Devigili e Maisa de O. Fraiz <br>\n",
    "# Projetos em Ciência de Dados - A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruções de Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis de configuração\n",
    "# Variável que define se o dataset será carregado de um .csv ou de um .pkl\n",
    "load_from_csv = False\n",
    "\n",
    "# Caminho para o dataset\n",
    "raw_data_path = '../data/raw-data/'\n",
    "dataset_path = raw_data_path + 'train_updated.csv'\n",
    "\n",
    "# Caminho onde serão salvos os dados processados\n",
    "processed_data_path = '../data/processed-data/'\n",
    "processed_dataset_path = processed_data_path + 'train.pkl' \n",
    "\n",
    "# Prepare data\n",
    "# Variável que define se o dataset será processado ou se será carregado de um .pkl\n",
    "prepare_data = True\n",
    "\n",
    "# Varíavel que define se vai realizar o feature selection ou vai carregar de um .pkl\n",
    "feature_selection = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "\n",
    "from functions import *\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variáveis globais\n",
    "\n",
    "PROCESSED_DATA_PATH = '../data/processed-data/'\n",
    "MODEL_PATH = '../models/trained-models/'\n",
    "\n",
    "TARGET_COLS = ['target1', 'target2', 'target3', 'target4']\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "TEST_SPLIT_DATE = '2021-04-30'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comentar que tá em outro arquivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga e Tratamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes\n",
    "dataset_names = {\n",
    "    'Awards': 'awards.csv', \n",
    "    'Example': 'example_test.csv', \n",
    "    'Players': 'players.csv',\n",
    "    'Seasons': 'seasons.csv', \n",
    "    'Teams': 'teams.csv', \n",
    "    'Train': 'train_updated.csv'\n",
    "}\n",
    "for key in dataset_names:\n",
    "    dataset_names[key] = raw_data_path + dataset_names[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o dataset Train Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if load_from_csv:\n",
    "    df_train = pd.read_csv(dataset_names['Train'])\n",
    "    pd.to_pickle(df_train, raw_data_path + 'train.pkl')\n",
    "else:\n",
    "    df_train = pd.read_pickle(raw_data_path + 'train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if prepare_data:\n",
    "\n",
    "    # criação do dataset de targets\n",
    "\n",
    "    # unpack the data\n",
    "    Y = unpack_data(df_train, dfs = ['nextDayPlayerEngagement'])['nextDayPlayerEngagement']\n",
    "\n",
    "    # change datatypes\n",
    "    Y = Y.astype({name: np.float32 for name in [\"target1\", \"target2\", \"target3\", \"target4\"]})\n",
    "\n",
    "    # match target dates to feature dates and create date index\n",
    "    Y = Y.rename(columns={'engagementMetricsDate': 'date'})\n",
    "\n",
    "    # change datatypes\n",
    "    Y['date'] = pd.to_datetime(Y['date'])\n",
    "\n",
    "    # reset index\n",
    "    Y = Y.set_index('date').to_period('D')\n",
    "    Y.index = Y.index - 1\n",
    "    Y = Y.reset_index()\n",
    "\n",
    "    # rename and select columns\n",
    "    cols_Y = {\n",
    "        'date': 'Dt',\n",
    "        'playerId': 'IdPlayer',\n",
    "        'target1': 'target1',\n",
    "        'target2': 'target2',\n",
    "        'target3': 'target3',\n",
    "        'target4': 'target4'\n",
    "    }\n",
    "    Y = Y[list(cols_Y)]\n",
    "    Y.columns = list(cols_Y.values())\n",
    "    Y['Dt'] = Y['Dt'].astype('datetime64[ns]')\n",
    "    Y = create_id(Y, ['Dt', 'IdPlayer'], 'Player')\n",
    "\n",
    "    pd.to_pickle(Y, processed_data_path + 'targets.pkl')\n",
    "\n",
    "    del Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PlayerBoxScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 57s\n",
      "Compiler : 1.29 s\n",
      "Parser   : 647 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if prepare_data:\n",
    "    # load the data\n",
    "    df_playerBoxScores = unpack_data(df_train, dfs = ['playerBoxScores'])['playerBoxScores']\n",
    "\n",
    "    # Cria o dataset de jogos\n",
    "    cols = {\n",
    "        # columns related to other dimensions\n",
    "        'gamePk': 'IdGame',\n",
    "        'gameDate': 'DtGame',\n",
    "        'gameTimeUTC': 'DtGameUTC',\n",
    "        'playerId': 'IdPlayer',\n",
    "        'teamId': 'IdTeam',\n",
    "        'jerseyNum': 'NuJersey',\n",
    "        'positionCode': 'CdPosition',\n",
    "        # suggested column\n",
    "        'strikeOutsPitching': 'NuStrikeOutsPitching',\n",
    "    }  \n",
    "    # numeric columns\n",
    "    for numeric_col in list(df_playerBoxScores.columns[12:]):\n",
    "        # skip the columns that contains data about pitching due the amount of Nan values\n",
    "        if 'Pitching' not in numeric_col:\n",
    "            cols[numeric_col] = 'Nu' + numeric_col[0].upper() + numeric_col[1:] + '_Player'\n",
    "\n",
    "    df_playerBoxScores = df_playerBoxScores[list(cols)]\n",
    "    df_playerBoxScores.columns = list(cols.values())\n",
    "\n",
    "    # df_playerBoxScores['DtGame'] = df_playerBoxScores['DtGame'] + \" 00:00:00\"\n",
    "    df_playerBoxScores['DtGame'] = df_playerBoxScores['DtGame'].astype('datetime64[ns]')\n",
    "    df_playerBoxScores['DtGameUTC'] = df_playerBoxScores['DtGameUTC'].astype('datetime64[ns]')\n",
    "    df_playerBoxScores = create_id(df_playerBoxScores, ['DtGame', 'IdPlayer'], 'Player')\n",
    "    df_playerBoxScores = create_id(df_playerBoxScores, ['DtGame', 'IdTeam'], 'Team')\n",
    "    df_playerBoxScores = create_id(df_playerBoxScores, ['DtGame', 'IdGame'], 'Game')\n",
    "\n",
    "    # Salva o dataset\n",
    "    pd.to_pickle(df_playerBoxScores, processed_data_path + 'playerBoxScores.pkl')\n",
    "\n",
    "    del df_playerBoxScores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Team Box Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if prepare_data:\n",
    "    df_tbs = unpack_data(df_train, dfs = ['teamBoxScores'])['teamBoxScores']\n",
    "\n",
    "    cols = {\n",
    "        'gameDate': 'DtGame',\n",
    "        'teamId': 'IdTeam',\n",
    "        'home': 'FlgHome',\n",
    "        'gamePk': 'IdGame',\n",
    "    }\n",
    "\n",
    "    for numeric_col in list(df_tbs.columns[4:]):\n",
    "        cols[numeric_col] = 'Nu' + numeric_col[0].upper() + numeric_col[1:] + '_Team'\n",
    "\n",
    "    df_tbs = df_tbs[list(cols)]\n",
    "    df_tbs.columns = list(cols.values())\n",
    "\n",
    "    df_tbs['DtGame'] = df_tbs['DtGame'].astype('datetime64[ns]')\n",
    "    df_tbs = create_id(df_tbs, ['DtGame', 'IdTeam'], 'Team')\n",
    "    df_tbs = create_id(df_tbs, ['DtGame', 'IdGame'], 'Game')\n",
    "    df_tbs['FlgHome'] = df_tbs['FlgHome'].astype('bool')\n",
    "\n",
    "    pd.to_pickle(df_tbs, processed_data_path + 'teamBoxScores.pkl')\n",
    "\n",
    "    del df_tbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.45 s\n",
      "Compiler : 370 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if prepare_data:\n",
    "    df_games = unpack_data(df_train, dfs = ['games'])['games']\n",
    "\n",
    "    cols = {\n",
    "        'gamePk': 'IdGame',\n",
    "        'gameType': 'CdGameType',\n",
    "        'season': 'NuSeason',\n",
    "        'gameDate': 'DtGame',\n",
    "        'codedGameState': 'CdGameState',\n",
    "        'detailedGameState': 'CdGameState',\n",
    "        'isTie': 'FlgTie',\n",
    "        'gameNumber': 'NuGame',\n",
    "        'doubleHeader': 'CdDoubleHeader',\n",
    "        'dayNight': 'CdDayNight',\n",
    "        'scheduledInnings': 'NuScheduledInnings',\n",
    "        'gamesInSeries': 'NuGamesInSeries',\n",
    "        'homeId': 'IdHomeTeam',\n",
    "        'homeWins': 'NuWinsHomeTeam',\n",
    "        'homeLosses': 'NuLossesHomeTeam',\n",
    "        'homeWinPct': 'NuWinPctHomeTeam',\n",
    "        'homeWinner': 'FlgWinnerHomeTeam',\n",
    "        'homeScore': 'NuScoreHomeTeam',\n",
    "        'awayId': 'IdAwayTeam',\n",
    "        'awayWins': 'NuWinsAwayTeam',\n",
    "        'awayLosses': 'NuLossesAwayTeam',\n",
    "        'awayWinPct': 'NuWinPctAwayTeam',\n",
    "        'awayWinner': 'FlgWinnerAwayTeam',\n",
    "        'awayScore': 'NuScoreAwayTeam',\n",
    "    }\n",
    "\n",
    "    df_games = df_games[list(cols)]\n",
    "    df_games.columns = list(cols.values())\n",
    "\n",
    "    df_games['DtGame'] = df_games['DtGame'].astype('datetime64[ns]')\n",
    "    df_games = create_id(df_games, ['DtGame', 'IdGame'], 'Game')\n",
    "\n",
    "    pd.to_pickle(df_games, processed_data_path + 'games.pkl')\n",
    "\n",
    "    del df_games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if prepare_data:\n",
    "    df_targets = pd.read_pickle(processed_data_path + 'targets.pkl')\n",
    "    df_pbs = pd.read_pickle(processed_data_path + 'playerBoxScores.pkl')\n",
    "    df_tbs = pd.read_pickle(processed_data_path + 'teamBoxScores.pkl')\n",
    "    df_g = pd.read_pickle(processed_data_path + 'games.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduzindo memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 71.99 MB\n",
      "Memory usage after optimization is: 71.99 MB\n",
      "Decreased by 0.0%\n",
      "Memory usage of dataframe is 33.74 MB\n",
      "Memory usage after optimization is: 33.74 MB\n",
      "Decreased by 0.0%\n",
      "Memory usage of dataframe is 1.43 MB\n",
      "Memory usage after optimization is: 1.43 MB\n",
      "Decreased by 0.0%\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if prepare_data:\n",
    "    df_targets = reduce_mem_usage(df_targets);\n",
    "    df_pbs = reduce_mem_usage(df_pbs);\n",
    "    df_tbs = reduce_mem_usage(df_tbs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if prepare_data:\n",
    "    sort_df(df_targets)\n",
    "    df_train = shift_targets(df_targets, shift_vals=[1, 2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if prepare_data:\n",
    "    # Transform the datetime col into new features\n",
    "    df_train['DtYear'] = df_train['Dt'].dt.year\n",
    "    df_train['DtMonth'] = df_train['Dt'].dt.month\n",
    "    df_train['DtDay'] = df_train['Dt'].dt.day\n",
    "    df_train['DtDayOfWeek'] = df_train['Dt'].dt.dayofweek\n",
    "    df_train['DtDayOfYear'] = df_train['Dt'].dt.dayofyear\n",
    "    df_train['DtQuarter'] = df_train['Dt'].dt.quarter\n",
    "    # get the hour and minute from the PBS\n",
    "    df_pbs['DtHour'] = df_pbs['DtGameUTC'].dt.hour\n",
    "    df_pbs['DtMinute'] = df_pbs['DtGameUTC'].dt.minute\n",
    "    df_pbs['DtHour'][df_pbs['DtHour'] == 0] = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unindo os datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Player Box Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if prepare_data:\n",
    "    df_train = pd.merge(df_train, df_pbs, on=['IdDtPlayer'], how='left')\n",
    "\n",
    "# Substitui os valores Nan das seguintes colunas por 0\n",
    "    f = [c for c in df_train.columns if c not in ['IdGame', 'DtGame', 'DtGameUTC',\n",
    "    'IdPlayer_y','IdTeam','NuJersey','CdPosition', 'target1_shift_1', 'target2_shift_1',\n",
    "    'target3_shift_1','target1_shift_2', 'target3_shift_2', 'target4_shift_2',\n",
    "    'target1_shift_3','target2_shift_3','target3_shift_3','target4_shift_3',\n",
    "    'target1_shift_4','target2_shift_4','target3_shift_4','target4_shift_4',\n",
    "    'target1_shift_5','target2_shift_5','target3_shift_5','target4_shift_5',\n",
    "    'target1_shift_6','target2_shift_6','target3_shift_6','target4_shift_6',\n",
    "    'target1_shift_7','target2_shift_7','target3_shift_7','target4_shift_7']]\n",
    "\n",
    "    df_train[f] = df_train[f].fillna(0)        \n",
    "\n",
    "    # Remove os na das seguintes colunas\n",
    "    df_train = df_train.dropna(subset=[             \n",
    "        'target1_shift_1', 'target2_shift_1', 'target3_shift_1', 'target1_shift_2',\n",
    "        'target3_shift_2', 'target4_shift_2', 'target1_shift_3', 'target2_shift_3',\n",
    "        'target3_shift_3', 'target4_shift_3', 'target1_shift_4', 'target2_shift_4',\n",
    "        'target3_shift_4', 'target4_shift_4', 'target1_shift_5', 'target3_shift_5',\n",
    "        'target4_shift_5', 'target1_shift_6', 'target2_shift_6', 'target3_shift_6',\n",
    "        'target4_shift_6', 'target1_shift_7', 'target2_shift_7', 'target3_shift_7',\n",
    "        'target4_shift_7'])\n",
    "\n",
    "    df_train.rename(columns={'IdPlayer_x': 'IdPlayer'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Team Box Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if prepare_data:\n",
    "    df_train = pd.merge(df_train, df_tbs, on = ['IdDtTeam'], how = 'left')\n",
    "\n",
    "    # Substitui os valores Nan das seguintes colunas por 0\n",
    "    f = [\n",
    "        'FlgHome','NuFlyOuts_Team', 'NuGroundOuts_Team', 'NuRunsScored_Team',\n",
    "        'NuDoubles_Team', 'NuTriples_Team', 'NuHomeRuns_Team', 'NuStrikeOuts_Team',\n",
    "        'NuBaseOnBalls_Team', 'NuIntentionalWalks_Team', 'NuHits_Team', 'NuHitByPitch_Team',\n",
    "        'NuAtBats_Team', 'NuCaughtStealing_Team', 'NuStolenBases_Team', 'NuGroundIntoDoublePlay_Team',\n",
    "        'NuGroundIntoTriplePlay_Team', 'NuPlateAppearances_Team', 'NuTotalBases_Team', 'NuRbi_Team',\n",
    "        'NuLeftOnBase_Team', 'NuSacBunts_Team', 'NuSacFlies_Team', 'NuCatchersInterference_Team',\n",
    "        'NuPickoffs_Team', 'NuAirOutsPitching_Team', 'NuGroundOutsPitching_Team', 'NuRunsPitching_Team',\n",
    "        'NuDoublesPitching_Team', 'NuTriplesPitching_Team', 'NuHomeRunsPitching_Team',\n",
    "        'NuStrikeOutsPitching_Team', 'NuBaseOnBallsPitching_Team', 'NuIntentionalWalksPitching_Team',\n",
    "        'NuHitsPitching_Team', 'NuHitByPitchPitching_Team', 'NuAtBatsPitching_Team',\n",
    "        'NuCaughtStealingPitching_Team', 'NuStolenBasesPitching_Team', 'NuInningsPitched_Team',\n",
    "        'NuEarnedRuns_Team', 'NuBattersFaced_Team', 'NuOutsPitching_Team', 'NuHitBatsmen_Team',\n",
    "        'NuBalks_Team', 'NuWildPitches_Team', 'NuPickoffsPitching_Team', 'NuRbiPitching_Team',\n",
    "        'NuInheritedRunners_Team', 'NuInheritedRunnersScored_Team', 'NuCatchersInterferencePitching_Team',\n",
    "        'NuSacBuntsPitching_Team', 'NuSacFliesPitching_Team'\n",
    "    ]\n",
    "\n",
    "    df_train[f] = df_train[f].fillna(0)        \n",
    "\n",
    "    df_train = df_train.rename(columns={'IdDtGame_y': 'IdDtGame'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if prepare_data:\n",
    "    df_train = pd.merge(df_train, df_g, on = ['IdDtGame'], how = 'left')\n",
    "\n",
    "    f = [\n",
    "        'NuSeason', 'NuGame',\n",
    "        'NuScheduledInnings', 'NuGamesInSeries', 'NuWinsHomeTeam',\n",
    "        'NuLossesHomeTeam', 'NuWinPctHomeTeam', 'NuScoreHomeTeam',\n",
    "        'NuWinsAwayTeam', 'NuLossesAwayTeam', 'NuWinPctAwayTeam', 'NuScoreAwayTeam'\n",
    "    ]\n",
    "    df_train[f] = df_train[f].fillna(0)        \n",
    "\n",
    "    df_train = pd.get_dummies(df_train, columns = ['CdPosition', \"CdGameType\", \"CdGameState\", \"CdDoubleHeader\", \n",
    "                                        \"CdDayNight\", \"FlgWinnerHomeTeam\", \"FlgWinnerAwayTeam\",'FlgTie', 'FlgHome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropa colunas com vários valores Nan\n",
    "%%time\n",
    "if prepare_data:\n",
    "    df_train.drop([\n",
    "    'IdGame_x', 'DtGame_x', 'DtGameUTC', 'IdPlayer_y',\n",
    "    'IdTeam_x', 'IdTeam_y', \"NuGameTimeUTC_Team\", \"IdDtGame\", \"DtGame\", \"IdGame\",\n",
    "    'IdGame_y', 'NuJersey', \"DtGame_y\", \"IdHomeTeam\", \"IdAwayTeam\", \"IdDtPlayer\",\n",
    "    \"IdDtTeam\", \"IdDtGame_x\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 2190.92 MB\n",
      "Memory usage after optimization is: 981.16 MB\n",
      "Decreased by 55.2%\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if prepare_data:\n",
    "    df_train_reduced = reduce_mem_usage(df_train[df_train.columns.difference([TARGET_COLS])]);\n",
    "    df_train_reduced[TARGET_COLS]= df_train[TARGET_COLS]\n",
    "\n",
    "    df_train = df_train_reduced.copy()\n",
    "\n",
    "    del df_train_reduced\n",
    "\n",
    "    df_targets.to_pickle(processed_data_path + 'targets.pkl')\n",
    "    df_pbs.to_pickle(processed_data_path + 'playerBoxScores.pkl')\n",
    "    df_tbs.to_pickle(processed_data_path + 'teamBoxScores.pkl')\n",
    "    df_g.to_pickle(processed_data_path + 'games.pkl')\n",
    "\n",
    "    df_train.to_pickle(processed_data_path + 'train.pkl')\n",
    "    \n",
    "    del df_train, df_pbs, df_tbs, df_g, df_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lê os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(processed_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparação e Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arruma float16 para conseguir rodar baselines\n",
    "df_train[TARGET_COLS] = df_train[TARGET_COLS].astype('float', copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if feature_selection:\n",
    "    \n",
    "    df_train = df_train.drop(columns = ['IdPlayer'])\n",
    "\n",
    "    train, test, val = train_test_split(df_train)\n",
    "\n",
    "    train['Dt'] = pd.to_numeric(pd.to_datetime(train['Dt']))\n",
    "    test['Dt']= pd.to_numeric(pd.to_datetime(test['Dt']))\n",
    "    val['Dt'] = pd.to_numeric(pd.to_datetime(val['Dt']))\n",
    "\n",
    "    # defines the model and create the multioutput regressor\n",
    "    model = GradientBoostingRegressor(random_state=RANDOM_SEED, loss = 'absolute_error')\n",
    "    model_name = 'GradientBoostingRegressor | MultiOutput'\n",
    "    regressor = MultiOutputRegressor(model, n_jobs=-1)\n",
    "\n",
    "    # fit the model\n",
    "    regressor.fit(train.drop(columns=TARGET_COLS), train[TARGET_COLS])\n",
    "\n",
    "    # make predictions\n",
    "    y_pred = pd.DataFrame(regressor.predict(test.drop(columns=TARGET_COLS)), columns=TARGET_COLS)\n",
    "\n",
    "    for i in range(len(TARGET_COLS)):\n",
    "        plt.bar(range(len(regressor.estimators_[i].feature_importances_)), regressor.estimators_[i].feature_importances_)\n",
    "        plt.show()\n",
    "        for i in range(len(TARGET_COLS)):\n",
    "            plt.bar(range(len(regressor.estimators_[i].feature_importances_)), regressor.estimators_[i].feature_importances_)\n",
    "\n",
    "    selected_features = pd.array([])\n",
    "    for i in range(len(TARGET_COLS)):\n",
    "        features = np.where(regressor.estimators_[i].feature_importances_ >= 0.005)\n",
    "        selected_features = np.append(selected_features, train.drop(columns=TARGET_COLS).columns[features])\n",
    "        selected_features.extend(list(np.where(regressor.estimators_[i].feature_importances_ > 0.0001)))\n",
    "    np.where(regressor.estimators_[0].feature_importances_ > 0.0)\n",
    "    len(np.unique(selected_features))\n",
    "\n",
    "    with open('features.pkl', 'wb') as file:\n",
    "         pickle.dump(selected_features, file)\n",
    "    pickle.load(open('features.pkl', 'rb'))\n",
    "    \n",
    "else:\n",
    "    with open('features.pkl', 'rb') as f:\n",
    "        selected_features = pkl.load(f)\n",
    "    df_train = df_train[np.append(np.append(TARGET_COLS, selected_features), 'IdPlayer')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = train_test_split(df_train)\n",
    "\n",
    "train['Dt'] = pd.to_numeric(pd.to_datetime(train['Dt']))\n",
    "test['Dt']= pd.to_numeric(pd.to_datetime(test['Dt']))\n",
    "val['Dt'] = pd.to_numeric(pd.to_datetime(val['Dt']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(columns = ['model', 'target1 | AMAE', 'target2 | AMAE', 'target3 | AMAE', 'target4 | AMAE', 'average | AMAE', 'average | MAE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decisão de métrica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texto sobre a decisão de métrica (não tem necessidade de incluir o código testando as métricas, acho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 32s\n",
      "Compiler : 1.07 s\n",
      "Parser   : 158 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>target1 | AMAE</th>\n",
       "      <th>target2 | AMAE</th>\n",
       "      <th>target3 | AMAE</th>\n",
       "      <th>target4 | AMAE</th>\n",
       "      <th>average | AMAE</th>\n",
       "      <th>average | MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Média</td>\n",
       "      <td>6843.978819</td>\n",
       "      <td>6398.583724</td>\n",
       "      <td>7121.952787</td>\n",
       "      <td>6823.816072</td>\n",
       "      <td>5437.987665</td>\n",
       "      <td>1.606922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Média por Jogador</td>\n",
       "      <td>6414.161377</td>\n",
       "      <td>5524.151229</td>\n",
       "      <td>6761.46272</td>\n",
       "      <td>6119.308259</td>\n",
       "      <td>4964.079988</td>\n",
       "      <td>1.316354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mediana</td>\n",
       "      <td>6900.77556</td>\n",
       "      <td>6587.051921</td>\n",
       "      <td>7190.445719</td>\n",
       "      <td>6912.163273</td>\n",
       "      <td>5518.29097</td>\n",
       "      <td>1.018378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mediana por Jogador</td>\n",
       "      <td>6833.931392</td>\n",
       "      <td>6005.371015</td>\n",
       "      <td>7149.41096</td>\n",
       "      <td>6419.212379</td>\n",
       "      <td>5281.77421</td>\n",
       "      <td>0.945304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Média</td>\n",
       "      <td>6843.978819</td>\n",
       "      <td>6398.583724</td>\n",
       "      <td>7121.952787</td>\n",
       "      <td>6823.816072</td>\n",
       "      <td>5437.987665</td>\n",
       "      <td>1.606922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Média por Jogador</td>\n",
       "      <td>6414.161377</td>\n",
       "      <td>5524.151229</td>\n",
       "      <td>6761.46272</td>\n",
       "      <td>6119.308259</td>\n",
       "      <td>4964.079988</td>\n",
       "      <td>1.316354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mediana</td>\n",
       "      <td>6900.77556</td>\n",
       "      <td>6587.051921</td>\n",
       "      <td>7190.445719</td>\n",
       "      <td>6912.163273</td>\n",
       "      <td>5518.29097</td>\n",
       "      <td>1.018378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mediana por Jogador</td>\n",
       "      <td>6833.931392</td>\n",
       "      <td>6005.371015</td>\n",
       "      <td>7149.41096</td>\n",
       "      <td>6419.212379</td>\n",
       "      <td>5281.77421</td>\n",
       "      <td>0.945304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model target1 | AMAE target2 | AMAE target3 | AMAE  \\\n",
       "0                Média    6843.978819    6398.583724    7121.952787   \n",
       "1    Média por Jogador    6414.161377    5524.151229     6761.46272   \n",
       "2              Mediana     6900.77556    6587.051921    7190.445719   \n",
       "3  Mediana por Jogador    6833.931392    6005.371015     7149.41096   \n",
       "4                Média    6843.978819    6398.583724    7121.952787   \n",
       "5    Média por Jogador    6414.161377    5524.151229     6761.46272   \n",
       "6              Mediana     6900.77556    6587.051921    7190.445719   \n",
       "7  Mediana por Jogador    6833.931392    6005.371015     7149.41096   \n",
       "\n",
       "  target4 | AMAE average | AMAE average | MAE  \n",
       "0    6823.816072    5437.987665      1.606922  \n",
       "1    6119.308259    4964.079988      1.316354  \n",
       "2    6912.163273     5518.29097      1.018378  \n",
       "3    6419.212379     5281.77421      0.945304  \n",
       "4    6823.816072    5437.987665      1.606922  \n",
       "5    6119.308259    4964.079988      1.316354  \n",
       "6    6912.163273     5518.29097      1.018378  \n",
       "7    6419.212379     5281.77421      0.945304  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# naive\n",
    "naive = train[train['Dt']=='2021-04-30'].set_index('IdPlayer')[TARGET_COLS]\n",
    "\n",
    "# media\n",
    "media = train[TARGET_COLS].mean()\n",
    "media_por_jogador = train.groupby('IdPlayer')[TARGET_COLS].mean()\n",
    "\n",
    "# mediana\n",
    "mediana = train[TARGET_COLS].median()\n",
    "mediana_por_jogador = train.groupby('IdPlayer')[TARGET_COLS].median()\n",
    "\n",
    "summary = pd.DataFrame()\n",
    "temp = pd.DataFrame()\n",
    "\n",
    "for target in TARGET_COLS:\n",
    "    \n",
    "    y_true = test[target]\n",
    "   \n",
    "    mediapj_pred = test['IdPlayer'].map(media_por_jogador[target].to_dict())\n",
    "    medianapj_pred = test['IdPlayer'].map(mediana_por_jogador[target].to_dict())\n",
    "    naive_pred = test['IdPlayer'].map(naive[target].to_dict())\n",
    "  \n",
    "    mediana_pred = [mediana[target] for i in test.index]\n",
    "    media_pred = [media[target] for i in test.index]\n",
    "    \n",
    "\n",
    "    temp.loc['Média',target]  = mean_absolute_error(y_true,media_pred)\n",
    "    temp.loc['Média por Jogador',target]  = mean_absolute_error(y_true,mediapj_pred)\n",
    "    temp.loc['Mediana',target]  = mean_absolute_error(y_true,mediana_pred)\n",
    "    temp.loc['Mediana por Jogador',target]  = mean_absolute_error(y_true,medianapj_pred)\n",
    "    #temp.loc['Naive',target]  = mean_absolute_error(y_true,naive_pred)\n",
    "    summary.loc['Média',target + \" | AMAE\"]  =  AMAE(y_true,media_pred,show = False)\n",
    "    summary.loc['Média por Jogador',target + \" | AMAE\"]  =  AMAE(y_true,mediapj_pred,show = False)\n",
    "    summary.loc['Mediana',target + \" | AMAE\"]  =  AMAE(y_true,mediana_pred,show = False)\n",
    "    summary.loc['Mediana por Jogador',target + \" | AMAE\"]  =  AMAE(y_true,medianapj_pred,show = False)\n",
    "    #summary.loc['Naive',target + \" | AMAE\"]  =  AMAE(y_true,naive_pred,show = False)\n",
    "    \n",
    "summary['average | MAE'] = temp.mean(axis=1)\n",
    "summary['average | AMAE'] = summary.mean(axis=1)\n",
    "\n",
    "summary = summary.reset_index()\n",
    "summary = summary.rename(columns = {\"index\": \"model\"})\n",
    "\n",
    "df_results = df_results.append(summary, ignore_index = True)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos Lineares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns = ['IdPlayer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# defines the model and create the multioutput regressor\n",
    "model = Lasso(alpha=0.1, random_state=RANDOM_SEED)\n",
    "model_name = 'Lasso | MultiOutput'\n",
    "regressor = MultiOutputRegressor(model, n_jobs=-1)\n",
    "\n",
    "# fit the model\n",
    "regressor.fit(train.drop(columns=TARGET_COLS), train[TARGET_COLS])\n",
    "\n",
    "# make predictions\n",
    "y_pred = pd.DataFrame(regressor.predict(test.drop(columns=TARGET_COLS)), columns=TARGET_COLS)\n",
    "\n",
    "# evaluate the model\n",
    "mae = evaluate_mae(test[TARGET_COLS], y_pred)\n",
    "amae = evaluate_amae(test[TARGET_COLS], y_pred)\n",
    "result_dict = {**amae, **{f'average | MAE': mae['average | MAE']}}\n",
    "\n",
    "# save the results\n",
    "df_results = df_results.append({'model': model_name, **result_dict}, ignore_index=True)\n",
    "df_results[df_results['model'] == model_name]\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# defines the model and create the multioutput regressor\n",
    "model = Ridge(alpha=0.1, random_state=RANDOM_SEED)\n",
    "model_name = 'Ridge | MultiOutput'\n",
    "regressor = MultiOutputRegressor(model, n_jobs=-1)\n",
    "\n",
    "# fit the model\n",
    "regressor.fit(train.drop(columns=TARGET_COLS), train[TARGET_COLS])\n",
    "\n",
    "# make predictions\n",
    "y_pred = pd.DataFrame(regressor.predict(test.drop(columns=TARGET_COLS)), columns=TARGET_COLS)\n",
    "\n",
    "# evaluate the model\n",
    "mae = evaluate_mae(test[TARGET_COLS], y_pred)\n",
    "amae = evaluate_amae(test[TARGET_COLS], y_pred)\n",
    "result_dict = {**amae, **{f'average | MAE': mae['average | MAE']}}\n",
    "\n",
    "# save the results\n",
    "df_results = df_results.append({'model': model_name, **result_dict}, ignore_index=True)\n",
    "df_results[df_results['model'] == model_name]\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# defines the model and create the multioutput regressor\n",
    "model = ElasticNet(alpha=0.1, random_state=RANDOM_SEED)\n",
    "model_name = 'ElasticNet | MultiOutput'\n",
    "regressor = MultiOutputRegressor(model, n_jobs=-1)\n",
    "\n",
    "# fit the model\n",
    "regressor.fit(train.drop(columns=TARGET_COLS), train[TARGET_COLS])\n",
    "\n",
    "# make predictions\n",
    "y_pred = pd.DataFrame(regressor.predict(test.drop(columns=TARGET_COLS)), columns=TARGET_COLS)\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "mae = evaluate_mae(test[TARGET_COLS], y_pred)\n",
    "amae = evaluate_amae(test[TARGET_COLS], y_pred)\n",
    "result_dict = {**amae, **{f'average | MAE': mae['average | MAE']}}\n",
    "\n",
    "# save the results\n",
    "df_results = df_results.append({'model': model_name, **result_dict}, ignore_index=True)\n",
    "df_results[df_results['model'] == model_name]\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# defines the model and create the multioutput regressor\n",
    "model = GradientBoostingRegressor(random_state=RANDOM_SEED, loss = 'absolute_error')\n",
    "model_name = 'GradientBoostingRegressor | MultiOutput'\n",
    "regressor = MultiOutputRegressor(model, n_jobs=-1)\n",
    "\n",
    "# fit the model\n",
    "regressor.fit(train.drop(columns=TARGET_COLS), train[TARGET_COLS])\n",
    "\n",
    "# make predictions\n",
    "y_pred = pd.DataFrame(regressor.predict(test.drop(columns=TARGET_COLS)), columns=TARGET_COLS)\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "mae = evaluate_mae(test[TARGET_COLS], y_pred)\n",
    "amae = evaluate_amae(test[TARGET_COLS], y_pred)\n",
    "result_dict = {**amae, **{f'average | MAE': mae['average | MAE']}}\n",
    "\n",
    "# save the results\n",
    "df_results = df_results.append({'model': model_name, **result_dict}, ignore_index=True)\n",
    "df_results[df_results['model'] == model_name]\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Output Chaining\n",
    "\n",
    "[note] adicionar aqui o teste falho de multi output chaining, explicar porque deu errado e mostrar que os resultados de um target não dependem dos demais. Além disso adicionar a correlação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Multi Output Regressor e Modelos Finais"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
