{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fundação Getúlio Vargas - RJ <br>\n",
    "> Escola de Matemática Aplicada (EMAp) <br>\n",
    "> Graduação em Ciência de Dados e Inteligência Artificial <br>\n",
    "> Alunos: Gianlucca Devigili e Maisa O. Fraiz <br>\n",
    "# Projetos em Ciência de Dados - A1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruções de execução\n",
    "\n",
    "__(#1)__ De modo a tornar mais rápida a carga dos dados, o projeto utiliza uma cópia dos arquivos em formato pickle (`.pkl`). Para executar o projeto utilizando o o dataset em formato `.csv`, atribua o valor `True` para a variável `V_load_from_csv`\n",
    "\n",
    "**(#2)** Redefina a variável `raw_data_path` para o caminho até o arquivo `train_updated.csv`. Substitua o nome do arquivo caso necessário.\n",
    "\n",
    "**(#3)** Atribua o valor `True` para a variável `save_files` caso ainda não tenha os arquivos `.pkl` dos datasets auxiliares salvos. (Necessário apenas para a primeira execução da sessão de preparação de dados).\n",
    "\n",
    "**(#4)** Atribua o valor `True` para a variável `prepare_data` caso deseje executar a sessão de preparação de dados. (Necessário apenas para a primeira execução da sessão de preparação de dados). Caso contrário as variáveis serão carregadas a partir dos arquivos `.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Variáveis de configuração\n",
    "# (#1) Variável que define se o dataset será carregado de um .csv ou de um .pkl\n",
    "load_from_csv = True\n",
    "\n",
    "# Caminho para o dataset\n",
    "raw_data_path = '../data/raw-data/'\n",
    "dataset_path = raw_data_path + 'train_updated.csv'\n",
    "\n",
    "# Caminho onde serão salvos os dados processados\n",
    "processed_data_path = '../data/processed-data/'\n",
    "\n",
    "# Prepare data\n",
    "# (#4) Variável que define se o dataset será processado ou se será carregado de um .pkl\n",
    "prepare_data = True\n",
    "\n",
    "save_files = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variáveis globais\n",
    "\n",
    "PROCESSED_DATA_PATH = '../data/processed-data/'\n",
    "MODEL_PATH = '../models/trained-models/'\n",
    "\n",
    "TARGET_COLS = ['target1', 'target2', 'target3', 'target4']\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "TEST_SPLIT_DATE = '2021-04-30'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparação dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções Auxiliares\n",
    "\n",
    "Algumas funções utilizadas para a preparação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções auxiliares para carregar os dados\n",
    "def unpack_json(json_str):\n",
    "    return pd.DataFrame() if pd.isna(json_str) else pd.read_json(json_str)\n",
    "\n",
    "def unpack_data(data, dfs=None, n_jobs=-1):\n",
    "    if dfs is not None:\n",
    "        data = data.loc[:, dfs]\n",
    "    unnested_dfs = {}\n",
    "    for name, column in data.iteritems():\n",
    "        daily_dfs = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(unpack_json)(item) for date, item in column.iteritems())\n",
    "        df = pd.concat(daily_dfs)\n",
    "        unnested_dfs[name] = df\n",
    "    return unnested_dfs\n",
    "\n",
    "def create_id(df, id_cols, id_col_name, dt_col_name = 'Dt'):\n",
    "    df['Id' + dt_col_name + id_col_name] = df[id_cols].apply(lambda x: '_'.join(x.astype(str)), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# (#1)\n",
    "if load_from_csv:\n",
    "    df_train = pd.read_csv(dataset_path)\n",
    "else: \n",
    "    dataset_path = raw_data_path + 'train.pkl'\n",
    "    df_train = pd.read_pickle(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cria o Dataframe Targets\n",
    "\n",
    "Dataframe contendo as 4 variáveis _target_ bem como suas respectivas chaves `IdPlayer` e `Dt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Compiler : 126 ms\n",
      "Parser   : 659 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "if prepare_data:\n",
    "    # criação do dataset de targets\n",
    "    # unpack the data\n",
    "    Y = unpack_data(df_train, dfs = ['nextDayPlayerEngagement'])['nextDayPlayerEngagement']\n",
    "\n",
    "    # change datatypes\n",
    "    Y = Y.astype({name: np.float32 for name in [\"target1\", \"target2\", \"target3\", \"target4\"]})\n",
    "\n",
    "    # match target dates to feature dates and create date index\n",
    "    Y = Y.rename(columns={'engagementMetricsDate': 'date'})\n",
    "\n",
    "    # change datatypes\n",
    "    Y['date'] = pd.to_datetime(Y['date'])\n",
    "\n",
    "    # reset index\n",
    "    Y = Y.set_index('date').to_period('D')\n",
    "    Y.index = Y.index - 1\n",
    "    Y = Y.reset_index()\n",
    "\n",
    "    # rename and select columns\n",
    "    cols_Y = {\n",
    "        'date': 'Dt',\n",
    "        'playerId': 'IdPlayer',\n",
    "        'target1': 'target1',\n",
    "        'target2': 'target2',\n",
    "        'target3': 'target3',\n",
    "        'target4': 'target4'\n",
    "    }\n",
    "    Y = Y[list(cols_Y)]\n",
    "    Y.columns = list(cols_Y.values())\n",
    "    Y['Dt'] = Y['Dt'].astype('datetime64[ns]')\n",
    "    Y = create_id(Y, ['Dt', 'IdPlayer'], 'Player')\n",
    "\n",
    "    if save_files:\n",
    "        pd.to_pickle(Y, processed_data_path + 'targets.pkl')\n",
    "\n",
    "    del Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cria o Dataframe Player Box Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prepare_data:\n",
    "    # load the data\n",
    "    df_playerBoxScores = unpack_data(df_train, dfs = ['playerBoxScores'])['playerBoxScores']\n",
    "\n",
    "    # Cria o dataset de jogos\n",
    "    cols = {\n",
    "        # columns related to other dimensions\n",
    "        'gamePk': 'IdGame',\n",
    "        'gameDate': 'DtGame',\n",
    "        'gameTimeUTC': 'DtGameUTC',\n",
    "        'playerId': 'IdPlayer',\n",
    "        'teamId': 'IdTeam',\n",
    "        'jerseyNum': 'NuJersey',\n",
    "        'positionCode': 'CdPosition',\n",
    "        # suggested column\n",
    "        'strikeOutsPitching': 'NuStrikeOutsPitching',\n",
    "    }  \n",
    "    # numeric columns\n",
    "    for numeric_col in list(df_playerBoxScores.columns[12:]):\n",
    "        # skip the columns that contains data about pitching due the amount of Nan values\n",
    "        if 'Pitching' not in numeric_col:\n",
    "            cols[numeric_col] = 'Nu' + numeric_col[0].upper() + numeric_col[1:]\n",
    "\n",
    "    df_playerBoxScores['gameDate'] = df_playerBoxScores['gameDate'] + \" 00:00:00\"\n",
    "\n",
    "    df_playerBoxScores = df_playerBoxScores[list(cols)]\n",
    "    df_playerBoxScores.columns = list(cols.values())\n",
    "    df_playerBoxScores = create_id(df_playerBoxScores, ['DtGame', 'IdPlayer'], 'Player')\n",
    "\n",
    "    # Salva o dataset\n",
    "    if save_files:\n",
    "        pd.to_pickle(df_playerBoxScores, processed_data_path + 'playerBoxScores.pkl')\n",
    "    del df_playerBoxScores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação dos dados target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(processed_data_path + 'targets.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções auxiliares para o pré-processamento dos dados\n",
    "def sort_df(df: pd.DataFrame, columns: list = ['IdPlayer', 'Dt']) -> None:\n",
    "    \"\"\"Sort the dataframe by the columns passed as argument.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe to be sorted.\n",
    "        columns (list, optional): Columns to sort the dataframe. Defaults to ['IdPlayer', 'Dt'].\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    df.sort_values(by=columns, inplace=True)\n",
    "    # reset index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "def shift_targets(df, shift_vals: list = [1, 2, 3, 4, 5, 6, 7, 14, 30]):\n",
    "    \"\"\"Shift the targets by the values passed as argument.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe to be shifted.\n",
    "        shift_vals (list, optional): Values to shift the targets. Defaults to [1, 2, 3, 4, 5, 6, 7, 14, 30].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with the shifted targets.\n",
    "    \"\"\"\n",
    "    df_aux = pd.DataFrame()\n",
    "    # Iterate over players to make the shift only using the player data\n",
    "    for player in df['IdPlayer'].unique():\n",
    "        df_player = df[df['IdPlayer'] == player]\n",
    "        # Iterate over the pre-defined shift values\n",
    "        for shift_val in shift_vals:\n",
    "            # Iterate over the targets\n",
    "            for target in TARGET_COLS:\n",
    "                # Make the shift\n",
    "                df_player[f'{target}_shift_{shift_val}'] = df_player[target].shift(shift_val)\n",
    "        # Concatenate the player data with the rest of the data\n",
    "        df_aux = pd.concat([df_aux, df_player], axis=0)\n",
    "        # Remove the player data from memory\n",
    "        del df_player\n",
    "    # df.dropna(inplace=True)\n",
    "    return df_aux\n",
    "\n",
    "\n",
    "def train_test_split(\n",
    "    df: pd.DataFrame\n",
    "    ,test_split_date: str = TEST_SPLIT_DATE\n",
    "    ):\n",
    "    \"\"\"Split the dataframe into train and test sets.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe to be split.\n",
    "        test_split_date (str, optional): Date to split the dataframe. Defaults to TEST_SPLIT_DATE.\n",
    "    \"\"\"\n",
    "\n",
    "    train = df[(df.Dt <= \"2021-01-31\") & (df.Dt >= \"2018-01-01\")] \n",
    "    val = df[(df.Dt <= \"2021-04-30\") & (df.Dt >= \"2021-02-01\")] \n",
    "    test = df[(df.Dt <= \"2021-07-31\") & (df.Dt >= \"2021-05-01\")]\n",
    "    # train.to_csv('train.csv', index=None)\n",
    "    # val.to_csv('validation.csv', index=None) \n",
    "    # test.to_csv('test.csv', index=None) \n",
    "\n",
    "    return train, test, val\n",
    "\n",
    "\n",
    "def x_y_split(df: pd.DataFrame, target_cols: list = TARGET_COLS):\n",
    "    \"\"\"Split the dataframe into x and y sets.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe to be split.\n",
    "    \"\"\"\n",
    "    y = df[target_cols]\n",
    "    x = df.drop(target_cols, axis=1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordenação dos valores\n",
    "\n",
    "Os dados serão ordenados por jogador e então por data, para facilitar a criação das variáveis de _shift_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dt</th>\n",
       "      <th>IdPlayer</th>\n",
       "      <th>target1</th>\n",
       "      <th>target2</th>\n",
       "      <th>target3</th>\n",
       "      <th>target4</th>\n",
       "      <th>IdDtPlayer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>112526</td>\n",
       "      <td>0.055277</td>\n",
       "      <td>5.496109</td>\n",
       "      <td>0.025839</td>\n",
       "      <td>16.176470</td>\n",
       "      <td>2018-01-01 00:00:00_112526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>112526</td>\n",
       "      <td>0.060625</td>\n",
       "      <td>3.252914</td>\n",
       "      <td>0.030486</td>\n",
       "      <td>8.541353</td>\n",
       "      <td>2018-01-02 00:00:00_112526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>112526</td>\n",
       "      <td>0.029341</td>\n",
       "      <td>1.648352</td>\n",
       "      <td>0.032613</td>\n",
       "      <td>10.490111</td>\n",
       "      <td>2018-01-03 00:00:00_112526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>112526</td>\n",
       "      <td>0.014799</td>\n",
       "      <td>2.665894</td>\n",
       "      <td>0.087422</td>\n",
       "      <td>19.091467</td>\n",
       "      <td>2018-01-04 00:00:00_112526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>112526</td>\n",
       "      <td>0.083916</td>\n",
       "      <td>1.161002</td>\n",
       "      <td>0.024759</td>\n",
       "      <td>6.643879</td>\n",
       "      <td>2018-01-05 00:00:00_112526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Dt  IdPlayer   target1   target2   target3    target4  \\\n",
       "0 2018-01-01    112526  0.055277  5.496109  0.025839  16.176470   \n",
       "1 2018-01-02    112526  0.060625  3.252914  0.030486   8.541353   \n",
       "2 2018-01-03    112526  0.029341  1.648352  0.032613  10.490111   \n",
       "3 2018-01-04    112526  0.014799  2.665894  0.087422  19.091467   \n",
       "4 2018-01-05    112526  0.083916  1.161002  0.024759   6.643879   \n",
       "\n",
       "                   IdDtPlayer  \n",
       "0  2018-01-01 00:00:00_112526  \n",
       "1  2018-01-02 00:00:00_112526  \n",
       "2  2018-01-03 00:00:00_112526  \n",
       "3  2018-01-04 00:00:00_112526  \n",
       "4  2018-01-05 00:00:00_112526  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_df(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dt</th>\n",
       "      <th>IdPlayer</th>\n",
       "      <th>target1</th>\n",
       "      <th>target2</th>\n",
       "      <th>target3</th>\n",
       "      <th>target4</th>\n",
       "      <th>IdDtPlayer</th>\n",
       "      <th>target1_shift_1</th>\n",
       "      <th>target2_shift_1</th>\n",
       "      <th>target3_shift_1</th>\n",
       "      <th>...</th>\n",
       "      <th>target3_shift_5</th>\n",
       "      <th>target4_shift_5</th>\n",
       "      <th>target1_shift_6</th>\n",
       "      <th>target2_shift_6</th>\n",
       "      <th>target3_shift_6</th>\n",
       "      <th>target4_shift_6</th>\n",
       "      <th>target1_shift_7</th>\n",
       "      <th>target2_shift_7</th>\n",
       "      <th>target3_shift_7</th>\n",
       "      <th>target4_shift_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>112526</td>\n",
       "      <td>0.055277</td>\n",
       "      <td>5.496109</td>\n",
       "      <td>0.025839</td>\n",
       "      <td>16.176470</td>\n",
       "      <td>2018-01-01 00:00:00_112526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>112526</td>\n",
       "      <td>0.060625</td>\n",
       "      <td>3.252914</td>\n",
       "      <td>0.030486</td>\n",
       "      <td>8.541353</td>\n",
       "      <td>2018-01-02 00:00:00_112526</td>\n",
       "      <td>0.055277</td>\n",
       "      <td>5.496109</td>\n",
       "      <td>0.025839</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>112526</td>\n",
       "      <td>0.029341</td>\n",
       "      <td>1.648352</td>\n",
       "      <td>0.032613</td>\n",
       "      <td>10.490111</td>\n",
       "      <td>2018-01-03 00:00:00_112526</td>\n",
       "      <td>0.060625</td>\n",
       "      <td>3.252914</td>\n",
       "      <td>0.030486</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>112526</td>\n",
       "      <td>0.014799</td>\n",
       "      <td>2.665894</td>\n",
       "      <td>0.087422</td>\n",
       "      <td>19.091467</td>\n",
       "      <td>2018-01-04 00:00:00_112526</td>\n",
       "      <td>0.029341</td>\n",
       "      <td>1.648352</td>\n",
       "      <td>0.032613</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>112526</td>\n",
       "      <td>0.083916</td>\n",
       "      <td>1.161002</td>\n",
       "      <td>0.024759</td>\n",
       "      <td>6.643879</td>\n",
       "      <td>2018-01-05 00:00:00_112526</td>\n",
       "      <td>0.014799</td>\n",
       "      <td>2.665894</td>\n",
       "      <td>0.087422</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Dt  IdPlayer   target1   target2   target3    target4  \\\n",
       "0 2018-01-01    112526  0.055277  5.496109  0.025839  16.176470   \n",
       "1 2018-01-02    112526  0.060625  3.252914  0.030486   8.541353   \n",
       "2 2018-01-03    112526  0.029341  1.648352  0.032613  10.490111   \n",
       "3 2018-01-04    112526  0.014799  2.665894  0.087422  19.091467   \n",
       "4 2018-01-05    112526  0.083916  1.161002  0.024759   6.643879   \n",
       "\n",
       "                   IdDtPlayer  target1_shift_1  target2_shift_1  \\\n",
       "0  2018-01-01 00:00:00_112526              NaN              NaN   \n",
       "1  2018-01-02 00:00:00_112526         0.055277         5.496109   \n",
       "2  2018-01-03 00:00:00_112526         0.060625         3.252914   \n",
       "3  2018-01-04 00:00:00_112526         0.029341         1.648352   \n",
       "4  2018-01-05 00:00:00_112526         0.014799         2.665894   \n",
       "\n",
       "   target3_shift_1  ...  target3_shift_5  target4_shift_5  target1_shift_6  \\\n",
       "0              NaN  ...              NaN              NaN              NaN   \n",
       "1         0.025839  ...              NaN              NaN              NaN   \n",
       "2         0.030486  ...              NaN              NaN              NaN   \n",
       "3         0.032613  ...              NaN              NaN              NaN   \n",
       "4         0.087422  ...              NaN              NaN              NaN   \n",
       "\n",
       "   target2_shift_6  target3_shift_6  target4_shift_6  target1_shift_7  \\\n",
       "0              NaN              NaN              NaN              NaN   \n",
       "1              NaN              NaN              NaN              NaN   \n",
       "2              NaN              NaN              NaN              NaN   \n",
       "3              NaN              NaN              NaN              NaN   \n",
       "4              NaN              NaN              NaN              NaN   \n",
       "\n",
       "   target2_shift_7  target3_shift_7  target4_shift_7  \n",
       "0              NaN              NaN              NaN  \n",
       "1              NaN              NaN              NaN  \n",
       "2              NaN              NaN              NaN  \n",
       "3              NaN              NaN              NaN  \n",
       "4              NaN              NaN              NaN  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = shift_targets(df, shift_vals=[1, 2, 3, 4, 5, 6, 7])\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Junção do dataset playerBoxScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_playerBoxScores = pd.read_pickle(processed_data_path + 'playerBoxScores.pkl')\n",
    "\n",
    "df_join = pd.merge(df, df_playerBoxScores, on=['IdDtPlayer'], how='left')\n",
    "df_join.info(null_counts=True)\n",
    "\n",
    "# Substitui os valores Nan das seguintes colunas por 0\n",
    "f = [c for c in df_join.columns if c not in ['IdGame',\n",
    "                                              'DtGame',\n",
    "                                              'DtGameUTC',\n",
    "                                              'IdPlayer_y',\n",
    "                                              'IdTeam',\n",
    "                                              'NuJersey',\n",
    "                                              'CdPosition', \n",
    "                                              'target1_shift_1', \n",
    "                                              'target2_shift_1',\n",
    "                                              'target3_shift_1',\n",
    "                                              'target1_shift_2',\n",
    "                                              'target3_shift_2',\n",
    "                                              'target4_shift_2',\n",
    "                                              'target1_shift_3',\n",
    "                                              'target2_shift_3',\n",
    "                                              'target3_shift_3',\n",
    "                                              'target4_shift_3',\n",
    "                                              'target1_shift_4',\n",
    "                                              'target2_shift_4',\n",
    "                                              'target3_shift_4',\n",
    "                                              'target4_shift_4',\n",
    "                                              'target1_shift_5',\n",
    "                                              'target2_shift_5',\n",
    "                                              'target3_shift_5',\n",
    "                                              'target4_shift_5',\n",
    "                                              'target1_shift_6',\n",
    "                                              'target2_shift_6',\n",
    "                                              'target3_shift_6',\n",
    "                                              'target4_shift_6',\n",
    "                                              'target1_shift_7',\n",
    "                                              'target2_shift_7',\n",
    "                                              'target3_shift_7',\n",
    "                                              'target4_shift_7']]\n",
    "\n",
    "df_join[f] = df_join[f].fillna(0)        \n",
    "\n",
    "# Remove os na das seguintes colunas\n",
    "df_join = df_join.dropna(subset=[             \n",
    "    'target1_shift_1', \n",
    "    'target2_shift_1',\n",
    "    'target3_shift_1',\n",
    "    'target1_shift_2',\n",
    "    'target3_shift_2',\n",
    "    'target4_shift_2',\n",
    "    'target1_shift_3',\n",
    "    'target2_shift_3',\n",
    "    'target3_shift_3',\n",
    "    'target4_shift_3',\n",
    "    'target1_shift_4',\n",
    "    'target2_shift_4',\n",
    "    'target3_shift_4',\n",
    "    'target4_shift_4',\n",
    "    'target1_shift_5',\n",
    "    'target3_shift_5',\n",
    "    'target4_shift_5', \n",
    "    'target1_shift_6',\n",
    "    'target2_shift_6',\n",
    "    'target3_shift_6',\n",
    "    'target4_shift_6',\n",
    "    'target1_shift_7',\n",
    "    'target2_shift_7',\n",
    "    'target3_shift_7',\n",
    "    'target4_shift_7'])\n",
    "\n",
    "# Dropa colunas com vários valores Nan\n",
    "df_join.drop(['IdGame',\n",
    "             'DtGame',\n",
    "             'DtGameUTC',\n",
    "             'IdPlayer_y',\n",
    "             'IdTeam',\n",
    "             'NuJersey',\n",
    "             'CdPosition',\n",
    "             'IdDtPlayer'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "df_join.rename(columns={'IdPlayer_x': 'IdPlayer'}, inplace=True)\n",
    "\n",
    "del df_train, df_playerBoxScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2684030 entries, 7 to 2698456\n",
      "Data columns (total 80 columns):\n",
      " #   Column                    Dtype  \n",
      "---  ------                    -----  \n",
      " 0   Dt                        int64  \n",
      " 1   IdPlayer                  int64  \n",
      " 2   target1                   float32\n",
      " 3   target2                   float32\n",
      " 4   target3                   float32\n",
      " 5   target4                   float32\n",
      " 6   target1_shift_1           float32\n",
      " 7   target2_shift_1           float32\n",
      " 8   target3_shift_1           float32\n",
      " 9   target4_shift_1           float32\n",
      " 10  target1_shift_2           float32\n",
      " 11  target2_shift_2           float32\n",
      " 12  target3_shift_2           float32\n",
      " 13  target4_shift_2           float32\n",
      " 14  target1_shift_3           float32\n",
      " 15  target2_shift_3           float32\n",
      " 16  target3_shift_3           float32\n",
      " 17  target4_shift_3           float32\n",
      " 18  target1_shift_4           float32\n",
      " 19  target2_shift_4           float32\n",
      " 20  target3_shift_4           float32\n",
      " 21  target4_shift_4           float32\n",
      " 22  target1_shift_5           float32\n",
      " 23  target2_shift_5           float32\n",
      " 24  target3_shift_5           float32\n",
      " 25  target4_shift_5           float32\n",
      " 26  target1_shift_6           float32\n",
      " 27  target2_shift_6           float32\n",
      " 28  target3_shift_6           float32\n",
      " 29  target4_shift_6           float32\n",
      " 30  target1_shift_7           float32\n",
      " 31  target2_shift_7           float32\n",
      " 32  target3_shift_7           float32\n",
      " 33  target4_shift_7           float32\n",
      " 34  NuStrikeOutsPitching      float64\n",
      " 35  NuBattingOrder            float64\n",
      " 36  NuGamesPlayedBatting      float64\n",
      " 37  NuFlyOuts                 float64\n",
      " 38  NuGroundOuts              float64\n",
      " 39  NuRunsScored              float64\n",
      " 40  NuDoubles                 float64\n",
      " 41  NuTriples                 float64\n",
      " 42  NuHomeRuns                float64\n",
      " 43  NuStrikeOuts              float64\n",
      " 44  NuBaseOnBalls             float64\n",
      " 45  NuIntentionalWalks        float64\n",
      " 46  NuHits                    float64\n",
      " 47  NuHitByPitch              float64\n",
      " 48  NuAtBats                  float64\n",
      " 49  NuCaughtStealing          float64\n",
      " 50  NuStolenBases             float64\n",
      " 51  NuGroundIntoDoublePlay    float64\n",
      " 52  NuGroundIntoTriplePlay    float64\n",
      " 53  NuPlateAppearances        float64\n",
      " 54  NuTotalBases              float64\n",
      " 55  NuRbi                     float64\n",
      " 56  NuLeftOnBase              float64\n",
      " 57  NuSacBunts                float64\n",
      " 58  NuSacFlies                float64\n",
      " 59  NuCatchersInterference    float64\n",
      " 60  NuPickoffs                float64\n",
      " 61  NuInningsPitched          float64\n",
      " 62  NuSaveOpportunities       float64\n",
      " 63  NuEarnedRuns              float64\n",
      " 64  NuBattersFaced            float64\n",
      " 65  NuPitchesThrown           float64\n",
      " 66  NuBalls                   float64\n",
      " 67  NuStrikes                 float64\n",
      " 68  NuHitBatsmen              float64\n",
      " 69  NuBalks                   float64\n",
      " 70  NuWildPitches             float64\n",
      " 71  NuInheritedRunners        float64\n",
      " 72  NuInheritedRunnersScored  float64\n",
      " 73  NuSaves                   float64\n",
      " 74  NuHolds                   float64\n",
      " 75  NuBlownSaves              float64\n",
      " 76  NuAssists                 float64\n",
      " 77  NuPutOuts                 float64\n",
      " 78  NuErrors                  float64\n",
      " 79  NuChances                 float64\n",
      "dtypes: float32(32), float64(46), int64(2)\n",
      "memory usage: 1.3 GB\n"
     ]
    }
   ],
   "source": [
    "df_join.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão treino, teste e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2322747, 35), Test shape: (189612, 35), Val shape: (183429, 35)\n"
     ]
    }
   ],
   "source": [
    "train, test, val = train_test_split(df)\n",
    "print(f\"Train shape: {train.shape}, Test shape: {test.shape}, Val shape: {val.shape}\")\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2322747, 35), Test shape: (189612, 35), Val shape: (183429, 35)\n"
     ]
    }
   ],
   "source": [
    "train_join, test_join, val_join = train_test_split(df_join)\n",
    "print(f\"Train shape: {train.shape}, Test shape: {test.shape}, Val shape: {val.shape}\")\n",
    "\n",
    "del df_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(columns = ['model', 'target1', 'target2', 'target3', 'target4', 'average'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to train, predict and evaluate models\n",
    "def train_models(model, x_train, y_train):\n",
    "    \"\"\"Train a model for each target column\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn model\n",
    "        Model to be trained\n",
    "    x_train : pd.DataFrame\n",
    "        Training features\n",
    "    y_train : pd.DataFrame\n",
    "        Training targets\n",
    "    \n",
    "    Returns\n",
    "    \n",
    "    -------\n",
    "    list\n",
    "        List of trained models\n",
    "    \"\"\"\n",
    "\n",
    "    models = []\n",
    "    for target in TARGET_COLS:\n",
    "        model.fit(x_train, y_train[target])\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "\n",
    "def predict_targets(models, x_test):\n",
    "    \"\"\"Predict the targets for each model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    models : list\n",
    "        List of trained models\n",
    "    x_test : pd.DataFrame\n",
    "        Test features\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Predictions for each target column\n",
    "    \"\"\"\n",
    "\n",
    "    y_preds = pd.DataFrame(columns=TARGET_COLS)\n",
    "    for target, model in zip(TARGET_COLS, models):\n",
    "        y_preds[target] = model.predict(x_test)\n",
    "    return y_preds\n",
    "\n",
    "\n",
    "def evaluate_mae(y_true, y_pred):\n",
    "    \"\"\"Evaluate the mean absolute error for each target column and the average MAE\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : pd.DataFrame\n",
    "        True labels\n",
    "    y_pred : pd.DataFrame\n",
    "        Predictions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mean absolute error for each target column\n",
    "    \"\"\"\n",
    "    maes = {}\n",
    "    for target in TARGET_COLS:\n",
    "        mae = mean_absolute_error(y_true[target], y_pred[target])\n",
    "        maes[target] = mae\n",
    "    maes['average'] = np.mean(list(maes.values()))\n",
    "    return maes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models sem PlayerBoxScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = pd.concat([train, val], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = train_val[TARGET_COLS].mean()\n",
    "media_por_jogador = train_val.groupby('IdPlayer')[TARGET_COLS].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediana = train_val[TARGET_COLS].median()\n",
    "mediana_por_jogador = train_val.groupby('IdPlayer')[TARGET_COLS].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive = train_val[train_val['Dt']=='2021-04-30'].set_index('IdPlayer')[TARGET_COLS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame()\n",
    "\n",
    "for target in TARGET_COLS:\n",
    "    \n",
    "    y_true = test[target]\n",
    "    \n",
    "    mediapj_pred = test['IdPlayer'].map(media_por_jogador[target].to_dict())\n",
    "    medianapj_pred = test['IdPlayer'].map(mediana_por_jogador[target].to_dict())\n",
    "    naive_pred = test['IdPlayer'].map(naive[target].to_dict())\n",
    "    \n",
    "    mediana_pred = [mediana[target] for i in test.index]\n",
    "    media_pred = [media[target] for i in test.index]\n",
    "    \n",
    "\n",
    "    summary.loc['Média (sem PBS)',target]  = mean_absolute_error(y_true,media_pred)\n",
    "    summary.loc['Média por Jogador (sem PBS)',target]  = mean_absolute_error(y_true,mediapj_pred)\n",
    "    summary.loc['Mediana (sem PBS)',target]  = mean_absolute_error(y_true,mediana_pred)\n",
    "    summary.loc['Mediana por Jogador (sem PBS)',target]  = mean_absolute_error(y_true,medianapj_pred)\n",
    "    summary.loc['Naive (sem PBS)',target]  = mean_absolute_error(y_true,naive_pred)\n",
    "    \n",
    "summary['average'] = summary.mean(axis=1)\n",
    "\n",
    "summary = summary.reset_index()\n",
    "summary = summary.rename(columns = {\"index\": \"model\"})\n",
    "df_results = df_results.append(summary, ignore_index = True)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'DtGameUTC'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3621\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3622\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'DtGameUTC'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10236\\1778321403.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#val['Dt'] = pd.to_numeric(pd.to_datetime(val['Dt']))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DtGameUTC'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DtGameUTC'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DtGameUTC'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DtGameUTC'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DtGameUTC'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DtGameUTC'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3505\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3506\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3622\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3623\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3624\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3625\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'DtGameUTC'"
     ]
    }
   ],
   "source": [
    "train_join['Dt'] = pd.to_numeric(pd.to_datetime(train_join['Dt']))\n",
    "test_join['Dt']= pd.to_numeric(pd.to_datetime(test_join['Dt']))\n",
    "val_join['Dt'] = pd.to_numeric(pd.to_datetime(val_join['Dt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maisa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.006e+07, tolerance: 4.014e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\maisa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.214e+07, tolerance: 8.975e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10236\\851458008.py\u001b[0m in \u001b[0;36mtrain_models\u001b[1;34m(model, x_train, y_train)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mTARGET_COLS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1052\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                 \u001b[0mthis_Xy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m             _, this_coef, this_dual_gap, this_iter = self.path(\n\u001b[0m\u001b[0;32m   1055\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m                 \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py\u001b[0m in \u001b[0;36menet_path\u001b[1;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001b[0m\n\u001b[0;32m    646\u001b[0m             )\n\u001b[0;32m    647\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mprecompute\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m             model = cd_fast.enet_coordinate_descent(\n\u001b[0m\u001b[0;32m    649\u001b[0m                 \u001b[0mcoef_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Lasso(alpha=0.1, random_state=RANDOM_SEED)\n",
    "\n",
    "# train the models\n",
    "models = train_models(\n",
    "            model = model, \n",
    "            x_train = train_join.drop(TARGET_COLS, axis=1), \n",
    "            y_train = train_join[TARGET_COLS]\n",
    "        )\n",
    "\n",
    "# predict the targets for each trained model\n",
    "y_pred = predict_targets(models, test_join.drop(TARGET_COLS, axis=1))\n",
    "\n",
    "# evaluate the models\n",
    "mae = evaluate_mae(y_true = test_join[TARGET_COLS], y_pred = y_pred)\n",
    "\n",
    "# save the results\n",
    "df_results = df_results.append({'model': 'Lasso | alpha = 0.1 (com PBS)', **mae}, ignore_index=True)\n",
    "\n",
    "# delete the variables to save RAM\n",
    "del models, y_pred, mae\n",
    "\n",
    "# show the results\n",
    "df_results[df_results['model'] == 'Lasso | alpha = 0.1 (com PBS)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso com alpha escolhido por CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 9s\n",
      "Parser   : 2.88 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>target1</th>\n",
       "      <th>target2</th>\n",
       "      <th>target3</th>\n",
       "      <th>target4</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LassoCV</td>\n",
       "      <td>1.565586</td>\n",
       "      <td>1.888583</td>\n",
       "      <td>1.439029</td>\n",
       "      <td>1.45282</td>\n",
       "      <td>1.586505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model   target1   target2   target3  target4   average\n",
       "15  LassoCV  1.565586  1.888583  1.439029  1.45282  1.586505"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = LassoCV(random_state=RANDOM_SEED)\n",
    "\n",
    "# train the models\n",
    "models = train_models(\n",
    "            model = model, \n",
    "            x_train = train_join.drop(TARGET_COLS, axis=1), \n",
    "            y_train = train_join[TARGET_COLS]\n",
    "        )\n",
    "\n",
    "# predict the targets for each trained model\n",
    "y_pred = predict_targets(models, test_join.drop(TARGET_COLS, axis=1))\n",
    "\n",
    "# evaluate the models\n",
    "mae = evaluate_mae(y_true = test_join[TARGET_COLS], y_pred = y_pred)\n",
    "\n",
    "# save the results\n",
    "df_results = df_results.append({'model': 'LassoCV (com PSB)', **mae}, ignore_index=True)\n",
    "\n",
    "# delete the variables to save RAM\n",
    "del models, y_pred, mae\n",
    "\n",
    "# show the results\n",
    "df_results[df_results['model'] == 'LassoCV (com PSB)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha escolhido pelo LassoCV:\n",
    "model.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Ridge(random_state=RANDOM_SEED)\n",
    "\n",
    "# train the models\n",
    "models = train_models(\n",
    "            model = model, \n",
    "            x_train = train_join.drop(TARGET_COLS, axis=1), \n",
    "            y_train = train_join[TARGET_COLS]\n",
    "        )\n",
    "\n",
    "# predict the targets for each trained model\n",
    "y_pred = predict_targets(models, test_join.drop(TARGET_COLS, axis=1))\n",
    "\n",
    "# evaluate the models\n",
    "mae = evaluate_mae(y_true = test_join[TARGET_COLS], y_pred = y_pred)\n",
    "\n",
    "# save the results\n",
    "df_results = df_results.append({'model': 'Ridge (com PSB)', **mae}, ignore_index=True)\n",
    "\n",
    "# delete the variables to save RAM\n",
    "del models, y_pred, mae\n",
    "\n",
    "# show the results\n",
    "df_results[df_results['model'] == 'Ridge (com PSB)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = ElasticNet(random_state=RANDOM_SEED)\n",
    "\n",
    "# train the models\n",
    "models = train_models(\n",
    "            model = model, \n",
    "            x_train = train_join.drop(TARGET_COLS, axis=1), \n",
    "            y_train = train_join[TARGET_COLS]\n",
    "        )\n",
    "\n",
    "# predict the targets for each trained model\n",
    "y_pred = predict_targets(models, test_join.drop(TARGET_COLS, axis=1))\n",
    "\n",
    "# evaluate the models\n",
    "mae = evaluate_mae(y_true = test_join[TARGET_COLS], y_pred = y_pred)\n",
    "\n",
    "# save the results\n",
    "df_results = df_results.append({'model': 'ElasticNet (com PSB)', **mae}, ignore_index=True)\n",
    "\n",
    "# delete the variables to save RAM\n",
    "del models, y_pred, mae\n",
    "\n",
    "# show the results\n",
    "df_results[df_results['model'] == 'ElasticNet (com PSB)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO com feature selection e alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rfe_lasso = RFE(LASSO(alpha = 0.1))\n",
    "# train the models\n",
    "models = train_models(\n",
    "            model = rfe_lasso,\n",
    "            x_train = train_join.drop(TARGET_COLS, axis=1),\n",
    "            y_train = train_join[TARGET_COLS]\n",
    "        )\n",
    "\n",
    "# predict the targets for each trained model\n",
    "y_pred = predict_targets(models, test_join.drop(TARGET_COLS, axis=1))\n",
    "\n",
    "# evaluate the models\n",
    "mae = evaluate_mae(y_true = test_join[TARGET_COLS], y_pred = y_pred)\n",
    "\n",
    "# save the results\n",
    "df_results = df_results.append({'model': 'FeatureSelection', **mae}, ignore_index=True)\n",
    "\n",
    "# delete the variables to save RAM\n",
    "del models, y_pred, mae\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_lasso.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tree Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 43s, sys: 1.09 s, total: 12min 44s\n",
      "Wall time: 12min 44s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>target1</th>\n",
       "      <th>target2</th>\n",
       "      <th>target3</th>\n",
       "      <th>target4</th>\n",
       "      <th>average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>1.535279</td>\n",
       "      <td>1.712461</td>\n",
       "      <td>1.48524</td>\n",
       "      <td>1.119111</td>\n",
       "      <td>1.463023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model   target1   target2  target3   target4   average\n",
       "2  Decision Tree Regressor  1.535279  1.712461  1.48524  1.119111  1.463023"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = DecisionTreeRegressor(random_state=RANDOM_SEED)\n",
    "\n",
    "# train the models\n",
    "models = train_models(\n",
    "            model = model, \n",
    "            x_train = train.drop(TARGET_COLS, axis=1), \n",
    "            y_train = train[TARGET_COLS]\n",
    "        )\n",
    "\n",
    "# predict the targets for each trained model\n",
    "y_pred = predict_targets(models, test.drop(TARGET_COLS, axis=1))\n",
    "\n",
    "# evaluate the models\n",
    "mae = evaluate_mae(y_true = test[TARGET_COLS], y_pred = y_pred)\n",
    "\n",
    "# save the results\n",
    "df_results = df_results.append({'model': 'Decision Tree Regressor (sem PBS)', **mae}, ignore_index=True)\n",
    "\n",
    "# delete the variables to save RAM\n",
    "del models, y_pred, mae\n",
    "\n",
    "df_results[df_results['model'] == 'Decision Tree Regressor (sem PBS)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = DecisionTreeRegressor(random_state=RANDOM_SEED)\n",
    "\n",
    "# train the models\n",
    "models = train_models(\n",
    "            model = model, \n",
    "            x_train = train_join.drop(TARGET_COLS, axis=1), \n",
    "            y_train = train_join[TARGET_COLS]\n",
    "        )\n",
    "\n",
    "# predict the targets for each trained model\n",
    "y_pred = predict_targets(models, test_join.drop(TARGET_COLS, axis=1))\n",
    "\n",
    "# evaluate the models\n",
    "mae = evaluate_mae(y_true = test_join[TARGET_COLS], y_pred = y_pred)\n",
    "\n",
    "# save the results\n",
    "df_results = df_results.append({'model': 'Decision Tree Regressor (com PBS)', **mae}, ignore_index=True)\n",
    "\n",
    "# delete the variables to save RAM\n",
    "del models, y_pred, mae\n",
    "\n",
    "df_results[df_results['model'] == 'Decision Tree Regressor (com PBS)']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = GradientBoostingRegressor(random_state=RANDOM_SEED)\n",
    "\n",
    "# train the models\n",
    "models = train_models(\n",
    "            model = model,\n",
    "            x_train = train.drop(TARGET_COLS, axis=1),\n",
    "            y_train = train[TARGET_COLS]\n",
    "        )\n",
    "\n",
    "# predict the targets for each trained model\n",
    "y_pred = predict_targets(models, test.drop(TARGET_COLS, axis=1))\n",
    "\n",
    "# evaluate the models\n",
    "mae = evaluate_mae(y_true = test[TARGET_COLS], y_pred = y_pred)\n",
    "\n",
    "# save the results\n",
    "df_results = df_results.append({'model': 'Gradient Boosting Regressor (com PBS)', **mae}, ignore_index=True)\n",
    "\n",
    "# delete the variables to save RAM\n",
    "del models, y_pred, mae\n",
    "\n",
    "df_results[df_results['model'] == 'Gradient Boosting Regressor (com PBS)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
