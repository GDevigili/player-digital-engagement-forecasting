{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing Datasets\n",
    "\n",
    "Notebook que irá conter o código para o pré-processamento dos dados, nenhum tratamento/higienização é realizado neste notebook, apenas a extração dos dados e a transformação dos dataset em `.pkl` serão realizadas aqui.\n",
    "\n",
    "os dados \"crus\" estão localizados em `/src/data/raw-data` e os dados tratados estão salvos em `/src/data/processed-data`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "# file paths\n",
    "raw_data_path = '../data/raw-data/'\n",
    "processed_data_path = '../data/processed-data/'\n",
    "\n",
    "# dataframes\n",
    "dataset_names = {\n",
    "    'Awards': 'awards.csv', \n",
    "    'Example': 'example_test.csv', \n",
    "    'Players': 'players.csv',\n",
    "    'Seasons': 'seasons.csv', \n",
    "    'Teams': 'teams.csv', \n",
    "    'Train': 'train_updated.csv'\n",
    "}\n",
    "for key in dataset_names:\n",
    "  dataset_names[key] = raw_data_path + dataset_names[key]\n",
    "dataset_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções auxiliares para carregar os dados\n",
    "def unpack_json(json_str):\n",
    "    return pd.DataFrame() if pd.isna(json_str) else pd.read_json(json_str)\n",
    "\n",
    "def unpack_data(data, dfs=None, n_jobs=-1):\n",
    "    if dfs is not None:\n",
    "        data = data.loc[:, dfs]\n",
    "    unnested_dfs = {}\n",
    "    for name, column in data.iteritems():\n",
    "        daily_dfs = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(unpack_json)(item) for date, item in column.iteritems())\n",
    "        df = pd.concat(daily_dfs)\n",
    "        unnested_dfs[name] = df\n",
    "    return unnested_dfs\n",
    "\n",
    "def create_id(df, id_cols, id_col_name, dt_col_name = 'Dt'):\n",
    "    df['Id' + dt_col_name + id_col_name] = df[id_cols].apply(lambda x: '_'.join(x.astype(str)), axis=1)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Train Updated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train = pd.read_csv(dataset_names['Train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(df_train, raw_data_path + 'train.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# criação do dataset de targets\n",
    "# unpack the data\n",
    "Y = unpack_data(df_train, dfs = ['nextDayPlayerEngagement'])['nextDayPlayerEngagement']\n",
    "\n",
    "# change datatypes\n",
    "Y = Y.astype({name: np.float32 for name in [\"target1\", \"target2\", \"target3\", \"target4\"]})\n",
    "\n",
    "# match target dates to feature dates and create date index\n",
    "Y = Y.rename(columns={'engagementMetricsDate': 'date'})\n",
    "\n",
    "# change datatypes\n",
    "Y['date'] = pd.to_datetime(Y['date'])\n",
    "\n",
    "# reset index\n",
    "Y = Y.set_index('date').to_period('D')\n",
    "Y.index = Y.index - 1\n",
    "Y = Y.reset_index()\n",
    "\n",
    "# rename and select columns\n",
    "cols_Y = {\n",
    "    'date': 'Dt',\n",
    "    'playerId': 'IdPlayer',\n",
    "    'target1': 'target1',\n",
    "    'target2': 'target2',\n",
    "    'target3': 'target3',\n",
    "    'target4': 'target4'\n",
    "}\n",
    "Y = Y[list(cols_Y)]\n",
    "Y.columns = list(cols_Y.values())\n",
    "Y['Dt'] = Y['Dt'].astype('datetime64[ns]')\n",
    "Y = create_id(Y, ['Dt', 'IdPlayer'], 'Player')\n",
    "\n",
    "if save_files:\n",
    "    pd.to_pickle(Y, processed_data_path + 'targets.pkl')\n",
    "\n",
    "del Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player Box Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df_playerBoxScores = unpack_data(df_train, dfs = ['playerBoxScores'])['playerBoxScores']\n",
    "\n",
    "# Cria o dataset de jogos\n",
    "cols = {\n",
    "    # columns related to other dimensions\n",
    "    'gamePk': 'IdGame',\n",
    "    'gameDate': 'DtGame',\n",
    "    'gameTimeUTC': 'DtGameUTC',\n",
    "    'playerId': 'IdPlayer',\n",
    "    'teamId': 'IdTeam',\n",
    "    'jerseyNum': 'NuJersey',\n",
    "    'positionCode': 'CdPosition',\n",
    "    # suggested column\n",
    "    'strikeOutsPitching': 'NuStrikeOutsPitching',\n",
    "}  \n",
    "# numeric columns\n",
    "for numeric_col in list(df_playerBoxScores.columns[12:]):\n",
    "    # skip the columns that contains data about pitching due the amount of Nan values\n",
    "    if 'Pitching' not in numeric_col:\n",
    "        cols[numeric_col] = 'Nu' + numeric_col[0].upper() + numeric_col[1:]\n",
    "\n",
    "df_playerBoxScores['DtGame'] = df_playerBoxScores['DtGame'] + \" 00:00:00\"\n",
    "\n",
    "df_playerBoxScores = df_playerBoxScores[list(cols)]\n",
    "df_playerBoxScores.columns = list(cols.values())\n",
    "df_playerBoxScores = create_id(df_playerBoxScores, ['DtGame', 'IdPlayer'], 'Player')\n",
    "\n",
    "# Salva o dataset\n",
    "if save_files:\n",
    "    pd.to_pickle(df_playerBoxScores, processed_data_path + 'playerBoxScores.pkl')\n",
    "del df_playerBoxScores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Other datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
